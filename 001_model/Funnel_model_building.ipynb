{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3dfc28-a5be-4525-a7af-b48aa8e0f1ae",
   "metadata": {},
   "source": [
    "## This model is to assign a TOPIC to a given Email\n",
    "## Two steps:\n",
    "### 1. unsupervised learning -Topic modeling to label each email with a topic ( use LDA).\n",
    "### 2. supervised learning - Classification model trained on the labeled data from the step 1(Xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5e7a4f-bf99-4c00-86be-0fb4152f8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e4f4c0-b6e9-4970-8522-1e89caf219b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3e0daef-6403-4467-b974-82e94cb1a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sudaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sudaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk to clean the emails\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'gov','com'])\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ed9659-11a0-40d6-900d-bbc1166bffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Funnel Leasing/emails/emails/'\n",
    "# path_demo = '/Funnel Leasing/emails/emails/00001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcecf46b-aab0-483c-8511-8e35e7251f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce10ef-2cb4-4836-abf9-908b27e3dbf4",
   "metadata": {},
   "source": [
    "### take a look at an email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9caa3ed8-75cb-472a-a256-2491d2428f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_demo,'r') as f:\n",
    "#     a = email.message_from_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5c1314-9a6c-4955-9c5b-77fef832ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dd0b657-6340-461e-9172-cd283ff0c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# body = a.get_payload().replace('\\n', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac25ec63-c946-4892-8481-c101e89fd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a00e7c4-cbc0-4cad-90a2-8398651f21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the signiture\n",
    "\n",
    "# clean_body=re.sub(r'(\\w*\\s)?([B|b]est|[R|r]egards|Have a|[C|c]heers|[S|s]incerely|[T|t]ake care|Looking forward|Fond|Kind|Yours|----.*----)(\\s*.*)',\"\", body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df7c646-aca5-48bd-b9de-d7e3587e4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e8671-3c9d-49f0-8e17-4f5f2589f8d8",
   "metadata": {},
   "source": [
    "## STEP 1. TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a73f2a-4c28-4931-af78-1190637ac83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all emails and extract the email body\n",
    "import os\n",
    "corpus = []\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        email_raw = email.message_from_file(f) # load the email\n",
    "        body = email_raw.get_payload().replace('\\n', '') # extract the email body\n",
    "        body =re.sub(r'(\\w*\\s)?([B|b]est|[R|r]egards|\\\n",
    "                            Have a|[C|c]heers|[S|s]incerely|[T|t]ake care|\\\n",
    "                            Looking forward|Fond|Kind|Yours|----.*----)(\\s*.*)',\"\", body) # remove the email signiture\n",
    "        corpus.append(body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf272fb-cb95-4f0f-93ae-6ad7e86e74a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mark A. Cartwright (markc@emx.utexas.edu) wrote:: Well,: : 42 is 101010 binary, and who would forget that its the: answer to the Question of \"Life, the Universe, and Everything else.\": That is to quote Douglas Adams in a round about way.: : Of course the Question has not yet been discovered...^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^But it WAS discovered (sort of).  The question was \"What is 7 times 8?\"When Arthur Dent objected that this was, unfortunately, factuallyinaccurate, the effort to discover the question was begun all over.This last effort was, I believe, likely to take far longer thanthe lifespan of the universe, in fact several lifespans of same!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a0d56ff-ebbd-4a00-af48-19d362fad58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for Filter for only nouns\n",
    "def noun_only(x):\n",
    "    pos_comment = nltk.pos_tag(x)\n",
    "    filtered =[word[0] for word in pos_comment if word[1] in ['NN']]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c2e48f8-a582-4d5f-b6e7-0462518125ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the text and lemmatize the words\n",
    "def stem_tokenizer(text):\n",
    "    clean_corpus = []\n",
    "    for sentence in text:\n",
    "        lemmatizer = WordNetLemmatizer()       \n",
    "        sentence = re.sub(r\"[^A-Za-z]\", \" \", sentence)# remove everthing expect the words\n",
    "        words = re.sub(r'(^| ).( |$)',' ',sentence) # remove single character word\n",
    "        words = re.sub(r'(^| ).( |$)',' ',words).lower().split() # remove the remaining single charater\n",
    "        words = [word for word in words if word not in stop_words] # remove stopwords\n",
    "        words = [lemmatizer.lemmatize(word) for word in words] # lemmatize the words\n",
    "        words = noun_only(words) # keep only the nouns\n",
    "        clean_corpus.append(words)\n",
    "    return clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9dfe0cdf-0caa-4832-aad8-51c455f34057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and lemmatize the text\n",
    "corpus_clean = stem_tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a976f8d7-bdb9-499b-865e-164441bbddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Gensim for LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4e4b5ba-9731-4896-9c50-c665731ce5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary for LDA\n",
    "id2word = corpora.Dictionary(corpus_clean)\n",
    "id2word.filter_extremes(no_below=10, no_above=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be3fafc2-a5a3-4cf6-b04f-7725ae8a6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a mapping for each work and its frequency in the docments.\n",
    "mapping = [id2word.doc2bow(sentence) for sentence in corpus_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fede529-d514-4e21-aeb2-808f49c7299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=mapping,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=2022,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=30,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "#lda_model.save('lda_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0365afaa-4c9c-47e5-9e0c-2c1647c8fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"time\" + 0.010*\"thing\" + 0.010*\"way\" + 0.008*\"question\" + 0.008*\"day\" '\n",
      "  '+ 0.007*\"car\" + 0.006*\"apr\" + 0.006*\"god\" + 0.006*\"world\" + 0.006*\"fact\"'),\n",
      " (1,\n",
      "  '0.021*\"window\" + 0.019*\"ax\" + 0.014*\"problem\" + 0.012*\"file\" + '\n",
      "  '0.011*\"drive\" + 0.010*\"card\" + 0.010*\"program\" + 0.009*\"anyone\" + '\n",
      "  '0.009*\"work\" + 0.009*\"system\"'),\n",
      " (2,\n",
      "  '0.013*\"system\" + 0.013*\"space\" + 0.008*\"number\" + 0.008*\"information\" + '\n",
      "  '0.008*\"government\" + 0.007*\"chip\" + 0.007*\"phone\" + 0.006*\"mail\" + '\n",
      "  '0.006*\"work\" + 0.006*\"list\"'),\n",
      " (3,\n",
      "  '0.018*\"year\" + 0.016*\"game\" + 0.014*\"team\" + 0.010*\"player\" + 0.007*\"time\" '\n",
      "  '+ 0.007*\"season\" + 0.006*\"apr\" + 0.006*\"city\" + 0.005*\"play\" + 0.005*\"jew\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60792d-0612-48bd-a925-39f1ab8a0e9c",
   "metadata": {},
   "source": [
    "## topic 0: \"TIME\"; topic 1: \"COMPUTER\"; topic 2: \"INFORMATION\"; topic 3: \"SPORTS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6de1aefa-06db-4cde-a705-f5ec55d21281",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {0: \"TIME\", 1: \"COMPUTER\", 2: \"INFORMATION\", 3: \"SPORTS\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21a3419b-fe08-449d-a872-caa05660bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lda_topics = pd.DataFrame(lda_model.get_document_topics(mapping, minimum_probability=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec386341-d638-4afd-9984-c0b56cf96d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics = pd.DataFrame(corpus,columns = ['email'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a116ad55-37b2-4504-a3c2-0ffcc29ade67",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics['topic'] = lda_topics.apply(lambda x: np.array([s[1] for s in x]).argmax(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f7a4a6a-d04e-49d9-9745-d76cf6164040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               email  topic\n",
      "0  Hi ... Recently I found XV for MS-DOS in a sub...      1\n",
      "1  Mark A. Cartwright (markc@emx.utexas.edu) wrot...      0\n",
      "2  > Problem 1> > My father told me the following...      3\n",
      "3  I  have one of the original Powerbook 170's (w...      1\n",
      "4          My family has never been particularly ...      0\n"
     ]
    }
   ],
   "source": [
    "print(corpus_topics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "144b437d-0b49-4d5c-b787-300d1a5aa446",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics.to_csv('corpus_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f9105-1120-4fd8-941d-803ea3d03622",
   "metadata": {},
   "source": [
    "## STEP 2. CLASSIFICAITON MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b98444db-55ae-4b3a-a04b-75e3eaf4f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus_topics['email'] \n",
    "y = corpus_topics['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2db9358b-1e33-4a21-b05a-44de1c02732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()       \n",
    "    sentence = re.sub(r\"[^A-Za-z]\", \" \", text)# remove everthing expect the words\n",
    "    words = re.sub(r'(^| ).( |$)',' ',sentence) # remove single character word\n",
    "    words = re.sub(r'(^| ).( |$)',' ',words).lower().split() # remove the remaining single charater\n",
    "    words = [word for word in words if word not in stop_words] # remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] # lemmatize the words\n",
    "    words = noun_only(words) # keep only the nouns\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ce47d4a-6635-4d47-b7c7-9439ebda89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tfidf to account for the significance of each word in the document\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words,\n",
    "                        tokenizer=tokenizer,\n",
    "                        lowercase=True,\n",
    "                        max_df=0.5,\n",
    "                        min_df=5\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2946feb-0588-48f2-917e-1f972fdbd03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "67b32464-1806-49b8-96a2-726befd94c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:30:46] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(max_df=0.5, min_df=5,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...],\n",
       "                                 tokenizer=<function tokenizer at 0x00000222F4FA2D30...\n",
       "                              interaction_constraints='', learning_rate=0.1,\n",
       "                              max_delta_step=0, max_depth=4, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints='()',\n",
       "                              n_estimators=500, n_jobs=16, num_class=4,\n",
       "                              num_parallel_tree=1, objective='multi:softmax',\n",
       "                              predictor='auto', random_state=0, reg_alpha=0,\n",
       "                              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "                              tree_method='exact', validate_parameters=1,\n",
       "                              verbosity=None))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model_xgb = Pipeline([('tfidf', tfidf),\n",
    "                     ('xgb', XGBRegressor(objective='multi:softmax', n_estimators=500,num_class=4,\n",
    "                        learning_rate = 0.1, max_depth=4))])\n",
    "model_xgb.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f5be83d-3ab5-491b-9642-5fd6766e7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95      3188\n",
      "           1       0.98      0.95      0.96      1956\n",
      "           2       0.97      0.92      0.94      1714\n",
      "           3       0.98      0.90      0.94      1061\n",
      "\n",
      "    accuracy                           0.95      7919\n",
      "   macro avg       0.96      0.94      0.95      7919\n",
      "weighted avg       0.95      0.95      0.95      7919\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "testing classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83      1364\n",
      "           1       0.86      0.84      0.85       787\n",
      "           2       0.81      0.65      0.72       747\n",
      "           3       0.89      0.69      0.78       496\n",
      "\n",
      "    accuracy                           0.81      3394\n",
      "   macro avg       0.83      0.77      0.79      3394\n",
      "weighted avg       0.81      0.81      0.80      3394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"training classification_report\")\n",
    "print(classification_report(y_train, model_xgb.predict(X_train)))\n",
    "print('-----------------------------------------------------------------')\n",
    "print(\"testing classification_report\")\n",
    "print(classification_report(y_test, model_xgb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1a998a7-291c-4b06-be24-137c17cb14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e8f53b8-69fa-4184-b4e7-0b95b6b2230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_xgb, open('model_xgb.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
